{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba35f486",
   "metadata": {},
   "source": [
    "Prerequisite 1: Download and Store the CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58caf788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: ./weather_data/2015/99495199999.csv\n",
      "Downloaded: ./weather_data/2015/72429793812.csv\n",
      "Failed to download https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/2016/99495199999.csv. Status code: 404\n",
      "Downloaded: ./weather_data/2016/72429793812.csv\n",
      "Downloaded: ./weather_data/2017/99495199999.csv\n",
      "Downloaded: ./weather_data/2017/72429793812.csv\n",
      "Downloaded: ./weather_data/2018/99495199999.csv\n",
      "Downloaded: ./weather_data/2018/72429793812.csv\n",
      "Downloaded: ./weather_data/2019/99495199999.csv\n",
      "Downloaded: ./weather_data/2019/72429793812.csv\n",
      "Downloaded: ./weather_data/2020/99495199999.csv\n",
      "Downloaded: ./weather_data/2020/72429793812.csv\n",
      "Downloaded: ./weather_data/2021/99495199999.csv\n",
      "Downloaded: ./weather_data/2021/72429793812.csv\n",
      "Downloaded: ./weather_data/2022/99495199999.csv\n",
      "Downloaded: ./weather_data/2022/72429793812.csv\n",
      "Downloaded: ./weather_data/2023/99495199999.csv\n",
      "Downloaded: ./weather_data/2023/72429793812.csv\n",
      "Downloaded: ./weather_data/2024/99495199999.csv\n",
      "Downloaded: ./weather_data/2024/72429793812.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Define the base URLs\n",
    "base_url_1 = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/{}/99495199999.csv\"\n",
    "base_url_2 = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/{}/72429793812.csv\"\n",
    "\n",
    "# Define the range of years\n",
    "years = range(2015, 2025)\n",
    "\n",
    "# Base directory to save the downloaded files\n",
    "base_output_dir = \"./weather_data/\"\n",
    "\n",
    "# Loop through each year and download the CSV files for both datasets\n",
    "for year in years:\n",
    "    # Create a directory for each year\n",
    "    year_dir = os.path.join(base_output_dir, str(year))\n",
    "    os.makedirs(year_dir, exist_ok=True)\n",
    "    \n",
    "    # Download each file (Florida and Cincinnati)\n",
    "    for base_url, station_id in [(base_url_1, \"99495199999\"), (base_url_2, \"72429793812\")]:\n",
    "        url = base_url.format(year)\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Save the file in the appropriate year directory\n",
    "            file_path = os.path.join(year_dir, f\"{station_id}.csv\")\n",
    "            with open(file_path, \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"Downloaded: {file_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c046de3",
   "metadata": {},
   "source": [
    "Prerequisite 2: Clean the data preserving original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4aa3dd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to: ./cleaned_weather_data/2015/99495199999.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2015/72429793812.csv\n",
      "File not found: ./weather_data/2016/99495199999.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2016/72429793812.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2017/99495199999.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2017/72429793812.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2018/99495199999.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2018/72429793812.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2019/99495199999.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2019/72429793812.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2020/99495199999.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2020/72429793812.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2021/99495199999.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2021/72429793812.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2022/99495199999.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2022/72429793812.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2023/99495199999.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2023/72429793812.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2024/99495199999.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2024/72429793812.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base input and output directories\n",
    "base_input_dir = \"./weather_data/\"\n",
    "base_output_dir = \"./cleaned_weather_data/\"\n",
    "\n",
    "# Define the invalid value representations\n",
    "invalid_values = {\n",
    "#     \"TEMP\": 9999.9,\n",
    "#     \"DEWP\": 9999.9,\n",
    "#     \"SLP\": 9999.9,\n",
    "#     \"STP\": 9999.9,\n",
    "#     \"VISIB\": 999.9,\n",
    "#     \"WDSP\": 999.9,\n",
    "    \"MXSPD\": 999.9,\n",
    "#     \"GUST\": 999.9,\n",
    "    \"MAX\": 9999.9,\n",
    "#     \"MIN\": 9999.9,\n",
    "#     \"PRCP\": 99.99,\n",
    "#     \"SNDP\": 999.9\n",
    "}\n",
    "\n",
    "# Loop through each year directory\n",
    "for year in range(2015, 2025):\n",
    "    year_dir = os.path.join(base_input_dir, str(year))\n",
    "    \n",
    "    # Check if the year directory exists\n",
    "    if os.path.exists(year_dir):\n",
    "        # Loop through each file in the year directory\n",
    "        for station_id in [\"99495199999\", \"72429793812\"]:\n",
    "            file_path = os.path.join(year_dir, f\"{station_id}.csv\")\n",
    "            \n",
    "            # Check if the file exists\n",
    "            if os.path.exists(file_path):\n",
    "                # Read the CSV file into a DataFrame\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Filter out rows with invalid values\n",
    "                for column, invalid_value in invalid_values.items():\n",
    "                    df = df[df[column] != invalid_value]\n",
    "                \n",
    "                # Create the output directory for the year if it doesn't exist\n",
    "                output_year_dir = os.path.join(base_output_dir, str(year))\n",
    "                os.makedirs(output_year_dir, exist_ok=True)\n",
    "                \n",
    "                # Save the cleaned DataFrame to the new directory\n",
    "                cleaned_file_path = os.path.join(output_year_dir, f\"{station_id}.csv\")\n",
    "                df.to_csv(cleaned_file_path, index=False)\n",
    "                print(f\"Cleaned data saved to: {cleaned_file_path}\")\n",
    "            else:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Year directory not found: {year_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dac3a0",
   "metadata": {},
   "source": [
    "Question 2: Load the CSV files and display the count of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2fe5c924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 2015/99495199999, Count: 355\n",
      "Dataset: 2015/72429793812, Count: 365\n",
      "Dataset: 2016/72429793812, Count: 366\n",
      "Dataset: 2017/99495199999, Count: 283\n",
      "Dataset: 2017/72429793812, Count: 365\n",
      "Dataset: 2018/99495199999, Count: 363\n",
      "Dataset: 2018/72429793812, Count: 365\n",
      "Dataset: 2019/99495199999, Count: 345\n",
      "Dataset: 2019/72429793812, Count: 365\n",
      "Dataset: 2020/99495199999, Count: 365\n",
      "Dataset: 2020/72429793812, Count: 366\n",
      "Dataset: 2021/99495199999, Count: 104\n",
      "Dataset: 2021/72429793812, Count: 365\n",
      "Dataset: 2022/99495199999, Count: 259\n",
      "Dataset: 2022/72429793812, Count: 365\n",
      "Dataset: 2023/99495199999, Count: 276\n",
      "Dataset: 2023/72429793812, Count: 365\n",
      "Dataset: 2024/99495199999, Count: 133\n",
      "Dataset: 2024/72429793812, Count: 296\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"WeatherDataCount\").getOrCreate()\n",
    "\n",
    "# Base path to the weather data\n",
    "base_path = \"./weather_data/\"\n",
    "\n",
    "# Dictionary to hold the counts of datasets\n",
    "dataset_counts = {}\n",
    "\n",
    "# Loop through each year from 2015 to 2024\n",
    "for year in range(2015, 2025):\n",
    "    for station_code in ['99495199999', '72429793812']:  # Florida and Cincinnati\n",
    "        file_path = os.path.join(base_path, str(year), f\"{station_code}.csv\")\n",
    "        \n",
    "        # Load the CSV file if it exists\n",
    "        if os.path.exists(file_path):\n",
    "            df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "            count = df.count()  # Get the count of rows\n",
    "            dataset_counts[f\"{year}/{station_code}\"] = count\n",
    "\n",
    "# Display the counts of each dataset\n",
    "for dataset, count in dataset_counts.items():\n",
    "    print(f\"Dataset: {dataset}, Count: {count}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f355d64",
   "metadata": {},
   "source": [
    "Question 3: Find the hottest day (column MAX) for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf2e74c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+--------------------+----------+----+\n",
      "|YEAR|    STATION|                NAME|      DATE| MAX|\n",
      "+----+-----------+--------------------+----------+----+\n",
      "|2015|99495199999|SEBASTIAN INLET S...|2015-07-28|90.0|\n",
      "|2016|72429793812|CINCINNATI MUNICI...|2016-07-24|93.9|\n",
      "|2017|99495199999|SEBASTIAN INLET S...|2017-05-13|88.3|\n",
      "|2018|99495199999|SEBASTIAN INLET S...|2018-09-15|90.1|\n",
      "|2019|99495199999|SEBASTIAN INLET S...|2019-09-06|91.6|\n",
      "|2020|99495199999|SEBASTIAN INLET S...|2020-04-13|91.8|\n",
      "|2021|72429793812|CINCINNATI MUNICI...|2021-08-12|95.0|\n",
      "|2022|72429793812|CINCINNATI MUNICI...|2022-06-14|96.1|\n",
      "|2023|99495199999|SEBASTIAN INLET S...|2023-12-10|79.5|\n",
      "|2024|99495199999|SEBASTIAN INLET S...|2024-05-14|86.7|\n",
      "+----+-----------+--------------------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "# Base path to the weather data\n",
    "base_path = \"./cleaned_weather_data/\"\n",
    "\n",
    "# Initialize a dictionary to store the hottest days per year\n",
    "hottest_days = {}\n",
    "\n",
    "# Loop through the years to find the hottest day\n",
    "for year in range(2015, 2025):\n",
    "    year_dir = os.path.join(base_path, str(year))\n",
    "    for filename in os.listdir(year_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = spark.read.csv(os.path.join(year_dir, filename), header=True, inferSchema=True)\n",
    "            \n",
    "            # Check if the DataFrame is empty\n",
    "            if df.isEmpty():\n",
    "                continue  # Skip to the next file\n",
    "            \n",
    "            # Check if the \"MAX\" column exists\n",
    "            if \"MAX\" not in df.columns:\n",
    "                print(f\"The 'MAX' column does not exist in {filename}.\")\n",
    "                continue  # Skip to the next file\n",
    "            \n",
    "            # Find the hottest day for the current DataFrame\n",
    "            max_day = df.orderBy(F.desc(\"MAX\")).first()\n",
    "            \n",
    "            # Check if max_day is None\n",
    "            if max_day is not None:\n",
    "                # Store the hottest day only if the year is not already recorded\n",
    "                if year not in hottest_days:\n",
    "                    hottest_days[year] = (max_day.STATION, max_day.NAME, max_day.DATE, max_day.MAX)\n",
    "\n",
    "# Convert results to a DataFrame for display\n",
    "if hottest_days:\n",
    "    hottest_days_list = [(year, *data) for year, data in hottest_days.items()]\n",
    "    hottest_days_df = spark.createDataFrame(hottest_days_list, [\"YEAR\", \"STATION\", \"NAME\", \"DATE\", \"MAX\"])\n",
    "    hottest_days_df.show()\n",
    "else:\n",
    "    print(\"No hottest days found across the datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44816886",
   "metadata": {},
   "source": [
    "Question. 4: Find the coldest day (column MIN) for the month of March across all years (2015-2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9eb20762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+---+\n",
      "|    STATION|                NAME|      DATE|MIN|\n",
      "+-----------+--------------------+----------+---+\n",
      "|72429793812|CINCINNATI MUNICI...|2015-03-06|3.2|\n",
      "+-----------+--------------------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "march_data = []\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Coldest Day\").getOrCreate()\n",
    "\n",
    "# Base path to the weather data\n",
    "base_path = \"./cleaned_weather_data/\"\n",
    "\n",
    "# Loop through the years to collect March data\n",
    "for year in range(2015, 2025):\n",
    "    year_dir = os.path.join(base_path, str(year))\n",
    "    for filename in os.listdir(year_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            df = spark.read.csv(os.path.join(year_dir, filename), header=True, inferSchema=True)\n",
    "            \n",
    "            # Filter for March data\n",
    "            march_df = df.filter(df.DATE.contains('-03-'))\n",
    "            \n",
    "            if not march_df.isEmpty():\n",
    "                # Get the coldest day for March in the current DataFrame\n",
    "                coldest_day = march_df.orderBy(F.asc(\"MIN\")).first()\n",
    "                \n",
    "                # Append results\n",
    "                if coldest_day is not None:\n",
    "                    march_data.append((coldest_day.STATION, coldest_day.NAME, coldest_day.DATE, coldest_day.MIN))\n",
    "\n",
    "# Convert results to a DataFrame for display\n",
    "if march_data:\n",
    "    coldest_day_df = spark.createDataFrame(march_data, [\"STATION\", \"NAME\", \"DATE\", \"MIN\"])\n",
    "    # Sort by MIN to get the overall coldest day in March\n",
    "    overall_coldest_day = coldest_day_df.orderBy(F.asc(\"MIN\")).first()\n",
    "    overall_coldest_day_df = spark.createDataFrame([overall_coldest_day], [\"STATION\", \"NAME\", \"DATE\", \"MIN\"])\n",
    "    overall_coldest_day_df.show()  # Display only the overall coldest day\n",
    "else:\n",
    "    print(\"No March data found across the datasets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e722b047",
   "metadata": {},
   "source": [
    "Question 5: Find the year with the most precipitation for Cincinnati and Florida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9675e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cincinnati: STATION=72429793812, NAME=CINCINNATI MUNICIPAL AIRPORT LUNKEN FIELD, OH US, YEAR=2024, Mean of PRCP=5.546203389830502\n",
      "Florida: STATION=99495199999, NAME=SEBASTIAN INLET STATE PARK, FL US, YEAR=2017, Mean of PRCP=0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "annual_precipitation = []\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Most Precipitation\").getOrCreate()\n",
    "\n",
    "# Base path to the cleaned weather data\n",
    "base_path = \"./cleaned_weather_data/\"\n",
    "\n",
    "# Loop through the years to calculate mean precipitation\n",
    "for year in range(2015, 2025):\n",
    "    year_dir = os.path.join(base_path, str(year))\n",
    "    for filename in os.listdir(year_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = spark.read.csv(os.path.join(year_dir, filename), header=True, inferSchema=True)\n",
    "            \n",
    "            # Check if the DataFrame is empty\n",
    "            if df.isEmpty():\n",
    "                continue  # Skip to the next file\n",
    "            \n",
    "            # Check if the DataFrame contains the 'PRCP' column\n",
    "            if \"PRCP\" not in df.columns:\n",
    "                print(f\"'PRCP' column not found in {filename}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate mean of PRCP\n",
    "            mean_prcp = df.agg(F.mean(\"PRCP\").alias(\"Mean_PRCP\")).first().Mean_PRCP\n",
    "            \n",
    "            # Get station info\n",
    "            station_id = df.select(\"STATION\").first().STATION\n",
    "            station_name = df.select(\"NAME\").first().NAME\n",
    "            \n",
    "            # Append results\n",
    "            annual_precipitation.append((station_id, station_name, year, mean_prcp))\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "annual_precipitation_df = spark.createDataFrame(annual_precipitation, [\"STATION\", \"NAME\", \"YEAR\", \"Mean_PRCP\"])\n",
    "\n",
    "# Find the year with the most precipitation for each station\n",
    "cincinnati_max_prcp = annual_precipitation_df.filter(annual_precipitation_df.STATION == \"72429793812\") \\\n",
    "    .orderBy(F.desc(\"Mean_PRCP\")).first()\n",
    "\n",
    "florida_max_prcp = annual_precipitation_df.filter(annual_precipitation_df.STATION == \"99495199999\") \\\n",
    "    .orderBy(F.desc(\"Mean_PRCP\")).first()\n",
    "\n",
    "# Display the results\n",
    "if cincinnati_max_prcp:\n",
    "    print(f\"Cincinnati: STATION={cincinnati_max_prcp.STATION}, NAME={cincinnati_max_prcp.NAME}, YEAR={cincinnati_max_prcp.YEAR}, Mean of PRCP={cincinnati_max_prcp.Mean_PRCP}\")\n",
    "\n",
    "if florida_max_prcp:\n",
    "    print(f\"Florida: STATION={florida_max_prcp.STATION}, NAME={florida_max_prcp.NAME}, YEAR={florida_max_prcp.YEAR}, Mean of PRCP={florida_max_prcp.Mean_PRCP}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0d56b6",
   "metadata": {},
   "source": [
    "Question 6: Count the percentage of missing values for wind gust (column GUST) for Cincinnati and Florida in the year 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6d555a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station Code: 99495199999, Missing GUST Percentage in the year 2024: 100.00%\n",
      "Station Code: 72429793812, Missing GUST Percentage in the year 2024: 40.00%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Wind Gust Missing Values\").getOrCreate()\n",
    "\n",
    "# Base path to the cleaned weather data\n",
    "base_path = \"./cleaned_weather_data/2024/\"\n",
    "\n",
    "# Station codes for Florida and Cincinnati\n",
    "station_codes = ['99495199999', '72429793812']  # Florida, Cincinnati\n",
    "results = []\n",
    "\n",
    "# Loop through each station code\n",
    "for station_code in station_codes:\n",
    "    file_path = os.path.join(base_path, f\"{station_code}.csv\")\n",
    "    \n",
    "    # Load the CSV file if it exists\n",
    "    if os.path.exists(file_path):\n",
    "        df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        \n",
    "        # Count total rows and missing values in the GUST column\n",
    "        total_count = df.count()\n",
    "        missing_count = df.filter(df.GUST == 999.9).count()\n",
    "        \n",
    "        # Calculate the percentage of missing values\n",
    "        if total_count > 0:\n",
    "            missing_percentage = (missing_count / total_count) * 100\n",
    "        else:\n",
    "            missing_percentage = 0.0\n",
    "        \n",
    "        # Store the result for this station\n",
    "        results.append((station_code, missing_percentage))\n",
    "\n",
    "# Display the results\n",
    "for station_code, missing_percentage in results:\n",
    "    print(f\"Station Code: {station_code}, Missing GUST Percentage in the year 2024: {missing_percentage:.2f}%\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81cfcfb",
   "metadata": {},
   "source": [
    "Question 7: Find the mean, median, mode, and standard deviation of the temperature (column TEMP) for Cincinnati in each month for the year 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a857302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------+----+------------------+\n",
      "|MONTH|              Mean|Median|Mode|Standard Deviation|\n",
      "+-----+------------------+------+----+------------------+\n",
      "|    1| 37.94516129032259|  37.7|43.1| 8.345810873712928|\n",
      "|    2|  36.5896551724138|  36.0|25.9|  7.90159770587055|\n",
      "|    3|  49.0741935483871|  47.8|39.6| 8.779406500135623|\n",
      "|    4|51.779999999999994|  51.0|48.4| 7.313162436838541|\n",
      "|    5| 60.89032258064518|  63.7|73.9| 9.314768017820217|\n",
      "|    6| 72.54666666666667|  73.7|74.2| 4.899946047087439|\n",
      "|    7|              77.6|  77.9|72.5|  2.33794781806609|\n",
      "|    8| 73.34516129032258|  73.7|73.2| 3.487868375734898|\n",
      "|    9|              66.1|  65.8|60.6| 7.118262089331474|\n",
      "|   10|55.193548387096776|  54.0|51.1|  6.72869157582517|\n",
      "|   11|48.003333333333345|  47.7|47.7| 6.825938527529321|\n",
      "|   12| 35.99354838709677|  35.2|32.1| 6.642787340861814|\n",
      "+-----+------------------+------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import mean, col, stddev, expr\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Temperature Analysis\").getOrCreate()\n",
    "\n",
    "# Load the data\n",
    "df = spark.read.csv(\"./cleaned_weather_data/2020/72429793812.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Extract month from date (assuming there's a DATE column)\n",
    "df_cincinnati = df.withColumn(\"MONTH\", F.month(col(\"DATE\")))\n",
    "\n",
    "# Group by month and calculate statistics\n",
    "result = df_cincinnati.groupBy(\"MONTH\").agg(\n",
    "    mean(\"TEMP\").alias(\"Mean\"),\n",
    "    expr(\"percentile_approx(TEMP, 0.5)\").alias(\"Median\"),  # Median\n",
    "    F.mode(\"TEMP\").alias(\"Mode\"),  # Mode\n",
    "    stddev(\"TEMP\").alias(\"Standard Deviation\")\n",
    ")\n",
    "\n",
    "# Show results\n",
    "result.orderBy(\"MONTH\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a882b5dc",
   "metadata": {},
   "source": [
    "Question 8: Find the top 10 days with the lowest Wind Chill for Cincinnati in 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ee0d12d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|      DATE|         Wind Chill|\n",
      "+----------+-------------------+\n",
      "|2017-01-07|-0.4140156367932173|\n",
      "|2017-12-31| 2.0339767075993116|\n",
      "|2017-12-27|  3.820645509123832|\n",
      "|2017-12-28|  4.533355269061226|\n",
      "|2017-01-06|  4.868933041653884|\n",
      "|2017-01-08|  7.929748208036862|\n",
      "|2017-12-25| 14.285113218297408|\n",
      "|2017-12-30| 14.539211253038193|\n",
      "|2017-01-05| 14.748861828163854|\n",
      "|2017-12-26| 15.688977805634499|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, unix_timestamp, date_format\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Wind Chill Analysis\").getOrCreate()\n",
    "\n",
    "# Load the data\n",
    "df = spark.read.csv(\"./cleaned_weather_data/2017/72429793812.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Filter for TEMP < 50°F, and WDSP > 3 mph\n",
    "df_cincinnati = df.filter((col(\"TEMP\") < 50) & (col(\"WDSP\") > 3))\n",
    "\n",
    "# Calculate Wind Chill using the given formula\n",
    "df_cincinnati = df_cincinnati.withColumn(\n",
    "    \"Wind Chill\",\n",
    "    35.74 + (0.6215 * col(\"TEMP\")) - (35.75 * (col(\"WDSP\") ** 0.16)) + (0.4275 * col(\"TEMP\") * (col(\"WDSP\") ** 0.16))\n",
    ")\n",
    "\n",
    "# Add a date column for sorting\n",
    "# Assuming there's a DATE column, we format it to just keep the date part\n",
    "df_cincinnati = df_cincinnati.withColumn(\"DATE\", date_format(\"DATE\", \"yyyy-MM-dd\"))\n",
    "\n",
    "# Select relevant columns and sort by Wind Chill\n",
    "result = df_cincinnati.select(\"DATE\", \"Wind Chill\").orderBy(\"Wind Chill\").limit(10)\n",
    "\n",
    "# Show results\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500d5291",
   "metadata": {},
   "source": [
    "Question 9: Investigate how many days had extreme weather conditions for Florida (fog, rain, snow, etc.) using the FRSHTT column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1496639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of days with extreme weather conditions in Florida: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Extreme Weather Analysis for Florida\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the directory containing cleaned weather data\n",
    "base_directory = './cleaned_weather_data/'\n",
    "file_paths = []\n",
    "\n",
    "# Collect all relevant file paths for Florida\n",
    "for year in range(2015, 2025):  # Adjust the range as necessary\n",
    "    file_path = os.path.join(base_directory, str(year), '99495199999.csv')\n",
    "    if os.path.exists(file_path):\n",
    "        file_paths.append(file_path)\n",
    "\n",
    "# Load all the CSV files into a single DataFrame\n",
    "df = spark.read.csv(file_paths, header=True, inferSchema=True)\n",
    "# Count the number of days with extreme weather conditions\n",
    "extreme_weather_count = df.filter(col(\"FRSHTT\") != 0).count()\n",
    "\n",
    "# Show the result\n",
    "print(f\"Number of days with extreme weather conditions in Florida: {extreme_weather_count}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6402a0",
   "metadata": {},
   "source": [
    "Question 10: Predict the maximum Temperature for Cincinnati for November and December 2024, based on the previous 2 years of weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a2af9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 17:41:07 WARN Instrumentation: [51f9c0f7] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|MONTH|Max Predicted Temp|\n",
      "+-----+------------------+\n",
      "|   11| 65.71804308370848|\n",
      "|   12| 55.37627286049107|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, dayofyear, month, max as spark_max, when\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Weather Data Prediction\").getOrCreate()\n",
    "\n",
    "# Define base directory for your CSV files\n",
    "base_directory = './cleaned_weather_data'\n",
    "file_paths = []\n",
    "\n",
    "# Collect file paths for relevant years (2022, 2023) for station \"72429793812\"\n",
    "for year in [2022, 2023]:\n",
    "    file_path = os.path.join(base_directory, str(year), '72429793812.csv')\n",
    "    if os.path.exists(file_path):\n",
    "        file_paths.append(file_path)\n",
    "\n",
    "# Load all the CSV files into a single DataFrame\n",
    "historical_data = spark.read.csv(file_paths, header=True, inferSchema=True)\n",
    "\n",
    "# Filter data for November and December (months 11 and 12) and for station \"72429793812\"\n",
    "historical_df = historical_data.filter(\n",
    "    (col(\"STATION\") == \"72429793812\") & (month(\"DATE\").isin([11, 12]))\n",
    ")\n",
    "\n",
    "# Prepare the training data by adding the day of the year\n",
    "training_data = historical_df.withColumn(\"DAY_OF_YEAR\", dayofyear(\"DATE\"))\n",
    "\n",
    "# Assemble the features\n",
    "assembler = VectorAssembler(inputCols=[\"DAY_OF_YEAR\"], outputCol=\"features\")\n",
    "train_data = assembler.transform(training_data).select(\"features\", col(\"MAX\").alias(\"label\"))\n",
    "\n",
    "# Train the Linear Regression model\n",
    "lr = LinearRegression()\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Prepare data for predicting for each day in November and December 2024 (days 305 to 365 of the year)\n",
    "predictions_df = spark.createDataFrame(\n",
    "    [(day,) for day in range(305, 366)], [\"DAY_OF_YEAR\"]\n",
    ")\n",
    "\n",
    "# Transform the prediction data with the same assembler\n",
    "predictions = assembler.transform(predictions_df)\n",
    "\n",
    "# Generate predictions using the trained model\n",
    "predicted_temps = lr_model.transform(predictions)\n",
    "\n",
    "# Identify the maximum predicted temperature for November and December\n",
    "max_predictions = predicted_temps.withColumn(\n",
    "    \"MONTH\", when(col(\"DAY_OF_YEAR\") < 335, 11).otherwise(12)\n",
    ").groupBy(\"MONTH\").agg(spark_max(\"prediction\").alias(\"Max Predicted Temp\"))\n",
    "\n",
    "# Show the maximum temperature predictions for November and December 2024\n",
    "max_predictions.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabea901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel",
   "language": "python",
   "name": "ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
